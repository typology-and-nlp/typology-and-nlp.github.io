---
layout: default
---

A long-standing research goal in Natural Language Processing (NLP) is the development of robust language technology applicable across the world's languages. However, this goal has not been met yet. One of the main causes is the inherent variation of the cross-lingual data (with respect to categories and structures). This results in poor performances of NLP algorithms when applied to a large scale, as their design, training, and hyperparameter tuning suffer from language-specific biases.

Linguistic typology holds promise to solve this problem. This field of study compares all languages systematically, and documents their variation in publicly available databases. Typological information from these databases has already provided guidance to multilingual NLP algorithms for feature engineering and data selection/synthesis.

However, typological information has not yet been fully exploited, as experiments were mostly limited to few features (mostly related to word order) and to few tasks (mostly dependency parsing). Moreover, typological databases are wanting in coverage and their interpretable, absolute, discrete features cannot be integrated straightforwardly into the state-of-art NLP algorithms, which are opaque, contextual, and probabilistic.

Our workshop aims at bridging this gap by encouraging a tighter collaboration of the scientific communities from the areas of linguistic typology and multilingual NLP. Moreover, we aim at fostering research in the following open problems:

1. Achievement of actual language-independence in algorithmic design.
2. Integration of typological features in joint multilingual / language transfer settings.
3. Automatic inference of typological features.
4. Interpretation of representations within multilingual neural models through typological knowledge.

---

## Invited Speakers

[Emily Bender](http://faculty.washington.edu/ebender/)'s primary research interests are in multilingual grammar engineering, the study of variation, both within and across languages, and the relationship between linguistics and computational linguistics. She is the LSA's delegate to the ACL. Her 2013 book [*Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax*](http://dx.doi.org/10.2200/S00493ED1V01Y201303HLT020) aims to present linguistic concepts in an manner accessible to NLP practitioners.

[Jason Eisner](http://www.cs.jhu.edu/~jason/) works on machine learning, combinatorial algorithms, probabilistic models of linguistic structure, and declarative specification of knowledge and algorithms. His work addresses the question, "How can we appropriately formalize linguistic structure and discover it automatically?"

[Balthasar Bickel](https://www.comparativelinguistics.uzh.ch/en/bickel.html) aims at understanding the diversity of human language with rigorously tested causal models, i.e. at answering the question **what’s where why in language**. What structures are there, and how exactly do they vary? Engaged in both linguistic fieldwork and statistical modeling, he focuses on explaining universally consistent biases in the diachrony of grammar properties, biases that are independent of local historical events.

[Sabine Stoll](https://www.psycholinguistics.uzh.ch/en/stoll.html) questions how children can cope with the incredible variation exhibited in the approximately 6000–7000 languages spoken around the world. Her main focus is the interplay of innate biological factors (such as the capacity for pattern recognition and imitation) with idiosyncratic and culturally determined factors (such as for instance type and quantity of input). Her approach is radically empirical, based first and foremost on the quantitative analysis of large corpora that record how children learn diverse languages.

[Isabelle Augenstein](http://isabelleaugenstein.github.io) is a tenure-track assistant professor at the University of Copenhagen, Department of Computer Science since July 2017, affiliated with the Copenhagen NLP group and the Machine Learning Section, and work in the general areas of Statistical Natural Language Processing and Machine Learning. Her main research interests are weakly supervised and low-resource learning with applications including information extraction, machine reading and fact checking.
